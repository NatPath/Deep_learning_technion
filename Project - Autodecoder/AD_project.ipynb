{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import create_dataloaders\n",
    "import AutoDecoder as AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'AutoDecoder' from '/home/nativ/Deep_learning_technion/Project - Autodecoder/AutoDecoder.py'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(AD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters\n",
    "latent_dim = 64  # Dimension of latent space\n",
    "output_shape = (1, 28, 28)  # Output shape for Fashion MNIST images\n",
    "\n",
    "# Initialize model\n",
    "auto_decoder = AD.AutoDecoder(latent_dim, output_shape).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_ds, train_dl, _, _ = create_dataloaders(data_path=\"dataset\",batch_size=32)\n",
    "\n",
    "# Initialize latent vectors for each training sample\n",
    "latents = torch.randn(len(train_ds), latent_dim, requires_grad=True, device=device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(list(auto_decoder.parameters()) + [latents], lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoDecoder(\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=784, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nativ/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 28, 28])) that is different to the input size (torch.Size([32, 1, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/nativ/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8, 28, 28])) that is different to the input size (torch.Size([8, 1, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 8863.4672\n",
      "Epoch [2/100], Loss: 5720.7717\n",
      "Epoch [3/100], Loss: 5647.6447\n",
      "Epoch [4/100], Loss: 5632.9624\n",
      "Epoch [5/100], Loss: 5620.9723\n",
      "Epoch [6/100], Loss: 5613.0190\n",
      "Epoch [7/100], Loss: 5608.7977\n",
      "Epoch [8/100], Loss: 5608.7714\n",
      "Epoch [9/100], Loss: 5614.0381\n",
      "Epoch [10/100], Loss: 5628.4303\n",
      "Epoch [11/100], Loss: 5660.1285\n",
      "Epoch [12/100], Loss: 5732.0539\n",
      "Epoch [13/100], Loss: 5906.8501\n",
      "Epoch [14/100], Loss: 6195.9662\n",
      "Epoch [15/100], Loss: 5974.4543\n",
      "Epoch [16/100], Loss: 5680.1492\n",
      "Epoch [17/100], Loss: 5588.5032\n",
      "Epoch [18/100], Loss: 5580.3694\n",
      "Epoch [19/100], Loss: 5576.1891\n",
      "Epoch [20/100], Loss: 5571.4197\n",
      "Epoch [21/100], Loss: 5568.3270\n",
      "Epoch [22/100], Loss: 5566.7894\n",
      "Epoch [23/100], Loss: 5566.0038\n",
      "Epoch [24/100], Loss: 5565.5841\n",
      "Epoch [25/100], Loss: 5565.3837\n",
      "Epoch [26/100], Loss: 5565.3814\n",
      "Epoch [27/100], Loss: 5565.5761\n",
      "Epoch [28/100], Loss: 5565.9818\n",
      "Epoch [29/100], Loss: 5566.6395\n",
      "Epoch [30/100], Loss: 5567.6389\n",
      "Epoch [31/100], Loss: 5569.0114\n",
      "Epoch [32/100], Loss: 5570.8962\n",
      "Epoch [33/100], Loss: 5573.3205\n",
      "Epoch [34/100], Loss: 5576.5123\n",
      "Epoch [35/100], Loss: 5580.2181\n",
      "Epoch [36/100], Loss: 5585.3806\n",
      "Epoch [37/100], Loss: 5592.3884\n",
      "Epoch [38/100], Loss: 5601.5612\n",
      "Epoch [39/100], Loss: 5618.2248\n",
      "Epoch [40/100], Loss: 5652.2368\n",
      "Epoch [41/100], Loss: 5725.9816\n",
      "Epoch [42/100], Loss: 5883.2603\n",
      "Epoch [43/100], Loss: 6165.2958\n",
      "Epoch [44/100], Loss: 6934.8896\n",
      "Epoch [45/100], Loss: 9365.1210\n",
      "Epoch [46/100], Loss: 8942.3479\n",
      "Epoch [47/100], Loss: 7651.9584\n",
      "Epoch [48/100], Loss: 7753.2669\n",
      "Epoch [49/100], Loss: 6175.0208\n",
      "Epoch [50/100], Loss: 6150.8803\n",
      "Epoch [51/100], Loss: 5887.4752\n",
      "Epoch [52/100], Loss: 5741.4793\n",
      "Epoch [53/100], Loss: 5660.4070\n",
      "Epoch [54/100], Loss: 5620.2022\n",
      "Epoch [55/100], Loss: 5597.9362\n",
      "Epoch [56/100], Loss: 5582.2669\n",
      "Epoch [57/100], Loss: 5570.2333\n",
      "Epoch [58/100], Loss: 5561.3942\n",
      "Epoch [59/100], Loss: 5555.1334\n",
      "Epoch [60/100], Loss: 5550.7029\n",
      "Epoch [61/100], Loss: 5547.5159\n",
      "Epoch [62/100], Loss: 5545.1165\n",
      "Epoch [63/100], Loss: 5543.2983\n",
      "Epoch [64/100], Loss: 5541.8955\n",
      "Epoch [65/100], Loss: 5540.8081\n",
      "Epoch [66/100], Loss: 5539.9674\n",
      "Epoch [67/100], Loss: 5539.3082\n",
      "Epoch [68/100], Loss: 5538.7891\n",
      "Epoch [69/100], Loss: 5538.3838\n",
      "Epoch [70/100], Loss: 5538.0645\n",
      "Epoch [71/100], Loss: 5537.8141\n",
      "Epoch [72/100], Loss: 5537.6151\n",
      "Epoch [73/100], Loss: 5537.4589\n",
      "Epoch [74/100], Loss: 5537.3358\n",
      "Epoch [75/100], Loss: 5537.2413\n",
      "Epoch [76/100], Loss: 5537.1679\n",
      "Epoch [77/100], Loss: 5537.1131\n",
      "Epoch [78/100], Loss: 5537.0733\n",
      "Epoch [79/100], Loss: 5537.0436\n",
      "Epoch [80/100], Loss: 5537.0231\n",
      "Epoch [81/100], Loss: 5537.0090\n",
      "Epoch [82/100], Loss: 5536.9995\n",
      "Epoch [83/100], Loss: 5536.9939\n",
      "Epoch [84/100], Loss: 5536.9908\n",
      "Epoch [85/100], Loss: 5536.9888\n",
      "Epoch [86/100], Loss: 5536.9868\n",
      "Epoch [87/100], Loss: 5536.9832\n",
      "Epoch [88/100], Loss: 5536.9777\n",
      "Epoch [89/100], Loss: 5536.9701\n",
      "Epoch [90/100], Loss: 5536.9639\n",
      "Epoch [91/100], Loss: 5536.9617\n",
      "Epoch [92/100], Loss: 5536.9711\n",
      "Epoch [93/100], Loss: 5536.9915\n",
      "Epoch [94/100], Loss: 5537.0167\n",
      "Epoch [95/100], Loss: 5537.0353\n",
      "Epoch [96/100], Loss: 5537.0497\n",
      "Epoch [97/100], Loss: 5537.1053\n",
      "Epoch [98/100], Loss: 5537.3004\n",
      "Epoch [99/100], Loss: 5537.7462\n",
      "Epoch [100/100], Loss: 5538.4798\n"
     ]
    }
   ],
   "source": [
    "# Train the AutoDecoder\n",
    "AD.train_auto_decoder(auto_decoder, train_dl, optimizer, latents, device, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs236781-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
